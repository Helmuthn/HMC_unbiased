{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>This project represents a quick implementation of unbiased  Hamiltonian Monte Carlo (HMC).</p> <p>The algorithm comes from the paper:</p> <pre><code>J Heng, P E Jacob, \"Unbiased Hamiltonian Monte Carlo With Couplings\", \nBiometrika, Volume 106, Issue 2, June 2019, Pages 287\u2013302, \nhttps://doi.org/10.1093/biomet/asy074\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>The project depends on <code>jax</code> and <code>jaxtyping</code>. It is recommended that you install <code>jax</code> seperately according to their install instructions.</p> <p><code>HMC_unbiased</code> can be installed through pip using:</p> <pre><code>pip install git+https://github.com/Helmuthn/HMC_unbiased.git\n</code></pre>"},{"location":"#theory","title":"Theory","text":"<p>The general idea in the algorithm is to couple two HMC processes such that when they coincide, they can be used to produce unbiased estimates of the mean of the process. This class of methods takes advantage of a telescoping sum.</p> <p>First, construct two Markov chains $(X_i, Y_i)$ such that they both converge to a desired stationary distribution $\\pi$, have an identical marginal distribution for each step, and are coupled such that if $X_i = Y_{i+1}$ for some $i$, then $X_j = Y_{j+1}$ for all $j &gt; i$.</p> <p>As $X_i$ is asymptotically unbiased, let $\\tilde X$ be a random variable from the limiting distribution, and</p> <p>$$ \\begin{align} \\mathbb{E}\\left[\\tilde X\\right] &amp;= \\mathbb{E}\\left[X_i + \\sum_{j=i}^{\\infty}(X_{j+1} - X_{j})\\right]\\\\ &amp;= \\mathbb{E}\\left[X_i\\right] + \\sum_{j=i}^{\\infty}\\mathbb{E}\\left[X_{j+1} - X_{j}\\right]\\\\ &amp;= \\mathbb{E}\\left[X_i\\right] + \\sum_{j=i}^{\\infty}\\mathbb{E}\\left[Y_{j+1} - X_{j}\\right]\\\\ &amp;= \\mathbb{E}\\left[X_i\\right] + \\sum_{j=i}^{T}\\mathbb{E}\\left[Y_{j+1} - X_{j}\\right], \\end{align} $$ where $T$ is the meeting time of the two processes. Equation (3) comes from noting that $X_i$ and $Y_i$ have the same distribution by construction, and Equation (4) comes from the constraint that $Y_{j+1} = X_j$ for any timesteps after $T$.</p> <p>Beyond verifying technical assumptions described in the paper, the main challenge in this class of algorithms is to ensure that the chains meet in finite time with high probability.</p> <p>The authors do this through the incorporation of a maximally coupled random walk model. That is, with some probability $\\gamma$, rather than take a step of HMC, instead step according to a Metropolis-Hastings random walk with a proposal distribution being a pair of maximally coupled Gaussian distributions.</p>"},{"location":"#usage","title":"Usage","text":"<p>There are two main high-level functions in this implementation: <code>unbiased_HMC_chain</code> and <code>unbiased_HMC_step</code>. These functions construct a the full MCMC chain until convergence and an individual step, respectively.</p> <p>The general approach in this library is to begin with an unnormalized distribution, then construct the potential functions using <code>helpers.construct_potential</code>. While not a particularly useful application, in the following example, we use Hamiltonian Monte Carlo to approximate a multivariate Gaussian distribution.</p> <p>First, we begin with the required imports. We additionally import jit from jax to compile the target distribution.</p> <pre><code>from HMC_unbiased.helpers import construct_potential\nfrom HMC_unbiased.main import unbiased_HMC_chain\nfrom jax import jit\nimport jax.random as random\nimport jax.numpy as jnp\n</code></pre> <p>Next, define a target distribution from which we will attempt to estimate the mean. In this case, it will be an offset Gaussian distribution.</p> <pre><code>@jit\ndef distribution(x):\n    squared_distance = -jnp.sum(jnp.square(x - 1))\n    normalized_distance = squared_distance / 2\n    density = jnp.exp(normalized_distance)/ ((2*jnp.pi)**(x.shape[0]/2))\n    return density\n\npotential, potential_grad = construct_potential(distribution)\n</code></pre> <p>We then define the hyperparameters for the Markov chains.</p> <pre><code>step_size = 0.05    # Leap-frog integrator step size\nnum_steps = 10      # Number of leap-frog steps per HMC step\ngamma = 0.1         # Probability of random walk step\nstd = 0.2           # Standard Deviation of random walk step\n</code></pre> <p>Choose some initialization for the Markov chains.</p> <pre><code>key = random.PRNGKey(1234)\nkey, subkey1, subkey2 = random.split(key, 3)\nQ1 = random.normal(subkey1, (5,))\nQ2 = random.normal(subkey2, (5,))\n</code></pre> <p>Finally, we can construct our sample chains.</p> <pre><code>Q1, Q2, chain_length = unbiased_HMC_chain(Q1, Q2,\n                                          potential, potential_grad,\n                                          step_size, num_steps,\n                                          gamma,\n                                          std, distribution,\n                                          key)\n\nQ1 = Q1[:chain_length, :]\nQ2 = Q2[:chain_length, :]\n</code></pre>"},{"location":"api/","title":"API","text":""},{"location":"api/#main","title":"Main","text":""},{"location":"api/#HMC_unbiased.main.unbiased_HMC_chain","title":"unbiased_HMC_chain","text":"<pre><code>unbiased_HMC_chain(\n    Q1: Float[Array, \" dim\"],\n    Q2: Float[Array, \" dim\"],\n    potential: Callable[[Float[Array, \" dim\"]], Float],\n    potential_grad: Callable[\n        [Float[Array, \" dim\"]], Float[Array, \" dim\"]\n    ],\n    step_size: Float,\n    num_steps: int,\n    gamma: Float,\n    std: Float,\n    marginal: Callable[[Float[Array, \" dim\"]], Float],\n    key: Key,\n) -&gt; tuple[\n    Float[Array, \"dim2 dim\"], Float[Array, \"dim3 dim\"], int\n]\n</code></pre> <p>Generate a sample chain from unbiased HMC.</p> <p>Constructs a sample chain from unbiased HMC with a given initialization.</p> <p>Parameters:</p> Name Type Description Default <code>Q1</code> <code>Float[Array, ' dim']</code> <p>Current state of the first Markov chain</p> required <code>Q2</code> <code>Float[Array, ' dim']</code> <p>Current state of the second Markov chain</p> required <code>potential</code> <code>Callable[[Float[Array, ' dim']], Float]</code> <p>Potential function</p> required <code>potential_grad</code> <code>Callable[[Float[Array, ' dim']], Float[Array, ' dim']]</code> <p>Gradient of the potential function</p> required <code>step_size</code> <code>Float</code> <p>Step size for leap-frog integration in HMC</p> required <code>num_steps</code> <code>int</code> <p>Number of steps for leap-frog integration in HMC</p> required <code>gamma</code> <code>Float</code> <p>Probability of selecting random walk </p> required <code>std</code> <code>Float</code> <p>Standard deviation for random walk</p> required <code>marginal</code> <code>Callable[[Float[Array, ' dim']], Float]</code> <p>Target marginal distribution</p> required <code>key</code> <code>Key</code> <p>Jax pseudorandom number key</p> required <p>Returns:</p> Type Description <code>tuple[Float[Array, 'dim2 dim'], Float[Array, 'dim3 dim'], int]</code> <p>A pair of sample chains</p> Source code in <code>HMC_unbiased/main.py</code> <pre><code>@partial(jax.jit, static_argnums=(2,3,8))\ndef unbiased_HMC_chain(Q1: Float[Array, \" dim\"], \n                       Q2: Float[Array, \" dim\"], \n                       potential: Callable[[Float[Array, \" dim\"]], Float],\n                       potential_grad: Callable[[Float[Array, \" dim\"]], \n                                                 Float[Array, \" dim\"]], \n                       step_size: Float, \n                       num_steps: int,\n                       gamma: Float,\n                       std: Float,\n                       marginal: Callable[[Float[Array, \" dim\"]], Float],\n                       key: Key\n                ) -&gt; tuple[Float[Array, \"dim2 dim\"], \n                           Float[Array, \"dim3 dim\"], \n                           int] :\n    \"\"\"Generate a sample chain from unbiased HMC.\n\n    Constructs a sample chain from unbiased HMC with a given initialization.\n\n    Args:\n        Q1: Current state of the first Markov chain\n        Q2: Current state of the second Markov chain\n        potential: Potential function\n        potential_grad: Gradient of the potential function\n        step_size: Step size for leap-frog integration in HMC\n        num_steps: Number of steps for leap-frog integration in HMC\n        gamma: Probability of selecting random walk \n        std: Standard deviation for random walk\n        marginal: Target marginal distribution\n        key: Jax pseudorandom number key\n\n    Returns:\n        A pair of sample chains\n    \"\"\"\n    out1 = jnp.zeros((10000, Q1.shape[0]))\n    out2 = jnp.zeros((10000, Q2.shape[0]))\n\n    out1 = out1.at[0,:].set(Q1)\n    out2 = out2.at[0,:].set(Q2)\n\n    args = (out1, out2, \n            step_size, num_steps, \n            std, \n            gamma,\n            key,\n            0)\n\n    def condition(args):\n        out1, out2, step_size, num_steps, std, gamma, key, i = args\n        return (out1[i,:] != out2[i,:]).all() &amp; (i &lt; 10000)\n\n\n    def loop_body(args):\n        out1, out2, step_size, num_steps, std, gamma, key, i = args\n        Q1 = out1[i,:]\n        Q2 = out2[i,:]\n        key, subkey = jax.random.split(key)\n        Q1, Q2 = unbiased_HMC_step(Q1, Q2, \n                                   potential, potential_grad, \n                                   step_size, num_steps, \n                                   gamma, std, marginal, subkey)\n        i += 1\n        out1 = out1.at[i,:].set(Q1)\n        out2 = out2.at[i,:].set(Q2)\n        return out1, out2, step_size, num_steps, std, gamma, key, i\n\n    args = jax.lax.while_loop(condition, loop_body, args)\n    out1 = args[0]\n    out2 = args[1]\n    i = args[7]\n\n    return out1, out2, i\n</code></pre>"},{"location":"api/#HMC_unbiased.main.time_averaged_estimate","title":"time_averaged_estimate","text":"<pre><code>time_averaged_estimate(\n    Q1: Float[Array, \"steps dim\"],\n    Q2: Float[Array, \"steps dim\"],\n    m: int,\n    k: int,\n) -&gt; Float[Array, \" dim\"]\n</code></pre> <p>Computes the time averaged estimate of expected value.</p> <p>Given two coupled chains from unbiased Hamiltonian Monte Carlo, computes the time averaged estimate.</p> <p>Parameters:</p> Name Type Description Default <code>Q1</code> <code>Float[Array, 'steps dim']</code> <p>First Chain</p> required <code>Q2</code> <code>Float[Array, 'steps dim']</code> <p>Second Chain</p> required <code>m</code> <code>int</code> <p>Burn-in time</p> required <code>k</code> <code>int</code> <p>Averaging step count</p> required <p>Returns:</p> Type Description <code>Float[Array, ' dim']</code> <p>The time averaged estimate.</p> Source code in <code>HMC_unbiased/main.py</code> <pre><code>@jax.jit\ndef time_averaged_estimate(Q1: Float[Array, \"steps dim\"], \n                           Q2: Float[Array, \"steps dim\"], \n                           m: int, \n                           k: int\n                        ) -&gt; Float[Array, \" dim\"]:\n    \"\"\"Computes the time averaged estimate of expected value.\n\n    Given two coupled chains from unbiased Hamiltonian Monte Carlo,\n    computes the time averaged estimate.\n\n    Args:\n        Q1: First Chain\n        Q2: Second Chain\n        m: Burn-in time\n        k: Averaging step count\n\n    Returns:\n        The time averaged estimate.\n    \"\"\"\n    # Remove the burn-in time of the Markov Chains\n    Q1 = Q1[m:, :]\n    Q2 = Q2[m:, :]\n\n    # Compute the averaging estimate from the first chain\n    average = jnp.sum(Q1[:k, :], axis=0)/k\n\n    # Compute the correction term\n    correction = jnp.sum(Q2[k:, :] - Q1[k:, :], axis=0)\n\n    return average + correction\n</code></pre>"},{"location":"api/#HMC_unbiased.main.unbiased_HMC_step","title":"unbiased_HMC_step","text":"<pre><code>unbiased_HMC_step(\n    Q1: Float[Array, \" dim\"],\n    Q2: Float[Array, \" dim\"],\n    potential: Callable[[Float[Array, \" dim\"]], Float],\n    potential_grad: Callable[\n        [Float[Array, \" dim\"]], Float[Array, \" dim\"]\n    ],\n    step_size: Float,\n    num_steps: int,\n    gamma: Float,\n    std: Float,\n    marginal: Callable[[Float[Array, \" dim\"]], Float],\n    key: Key,\n) -&gt; tuple[Float[Array, \" dim\"], Float[Array, \" dim\"]]\n</code></pre> <p>Completes a step of unbiased Hamiltonian Monte Carlo.</p> <p>Completes a step of the unbiased HMC process by randomly sleecting between HMC and a maximally coupled random walk Metropolis-Hastings process.</p> <p>Parameters:</p> Name Type Description Default <code>Q1</code> <code>Float[Array, ' dim']</code> <p>Current state of the first Markov chain</p> required <code>Q2</code> <code>Float[Array, ' dim']</code> <p>Current state of the second Markov chain</p> required <code>potential</code> <code>Callable[[Float[Array, ' dim']], Float]</code> <p>Potential function</p> required <code>potential_grad</code> <code>Callable[[Float[Array, ' dim']], Float[Array, ' dim']]</code> <p>Gradient of the potential function</p> required <code>step_size</code> <code>Float</code> <p>Step size for leap-frog integration in HMC</p> required <code>num_steps</code> <code>int</code> <p>Number of steps for leap-frog integration in HMC</p> required <code>gamma</code> <code>Float</code> <p>Probability of selecting random walk </p> required <code>std</code> <code>Float</code> <p>Standard deviation for random walk</p> required <code>marginal</code> <code>Callable[[Float[Array, ' dim']], Float]</code> <p>Target marginal distribution</p> required <code>key</code> <code>Key</code> <p>Jax pseudorandom number key</p> required <p>Returns:</p> Type Description <code>tuple[Float[Array, ' dim'], Float[Array, ' dim']]</code> <p>The result of an unbiased step of Hamiltonian Monte Carlo</p> Source code in <code>HMC_unbiased/main.py</code> <pre><code>@partial(jax.jit, static_argnums=(2,3,8))\ndef unbiased_HMC_step(Q1: Float[Array, \" dim\"], \n                      Q2: Float[Array, \" dim\"], \n                      potential: Callable[[Float[Array, \" dim\"]], Float],\n                      potential_grad: Callable[[Float[Array, \" dim\"]], \n                                                Float[Array, \" dim\"]], \n                      step_size: Float, \n                      num_steps: int,\n                      gamma: Float,\n                      std: Float,\n                      marginal: Callable[[Float[Array, \" dim\"]], Float],\n                      key: Key\n            ) -&gt; tuple[Float[Array, \" dim\"], Float[Array, \" dim\"]] :\n    \"\"\"Completes a step of unbiased Hamiltonian Monte Carlo.\n\n    Completes a step of the unbiased HMC process by randomly sleecting\n    between HMC and a maximally coupled random walk Metropolis-Hastings process.\n\n    Args:\n        Q1: Current state of the first Markov chain\n        Q2: Current state of the second Markov chain\n        potential: Potential function\n        potential_grad: Gradient of the potential function\n        step_size: Step size for leap-frog integration in HMC\n        num_steps: Number of steps for leap-frog integration in HMC\n        gamma: Probability of selecting random walk \n        std: Standard deviation for random walk\n        marginal: Target marginal distribution\n        key: Jax pseudorandom number key\n\n    Returns:\n        The result of an unbiased step of Hamiltonian Monte Carlo    \n    \"\"\"\n\n    key1, key2 = jax.random.split(key)\n    random_walk_choice = jax.random.bernoulli(key1, gamma)\n    args = (Q1, \n            Q2, \n            step_size, \n            num_steps, \n            std, \n            key2)\n\n    def random_walk(args):\n        Q1 = args[0]\n        Q2 = args[1]\n        std = args[4]\n        key = args[5]\n        return coupled_randomwalk_step(Q1, Q2, std, marginal, key)\n\n    def hmc(args):\n        Q1 = args[0]\n        Q2 = args[1]\n        step_size = args[2]\n        num_steps = args[3]\n        key = args[5]\n        return coupled_HMC_step(Q1, Q2, \n                                potential, potential_grad, \n                                step_size, num_steps, \n                                key)\n\n    return jax.lax.cond(random_walk_choice, random_walk, hmc, args)\n</code></pre>"},{"location":"api/#HMC_unbiased.main.coupled_HMC_step","title":"coupled_HMC_step","text":"<pre><code>coupled_HMC_step(\n    Q1: Float[Array, \" dim\"],\n    Q2: Float[Array, \" dim\"],\n    potential: Callable[[Float[Array, \" dim\"]], Float],\n    potential_grad: Callable[\n        [Float[Array, \" dim\"]], Float[Array, \" dim\"]\n    ],\n    step_size: Float,\n    num_steps: int,\n    key: Key,\n) -&gt; tuple[Float[Array, \" dim\"], Float[Array, \" dim\"]]\n</code></pre> <p>Completes a coupled step of Hamiltonian Monte Carlo.</p> <p>Given the current states of two Hamiltonian Monte Carlo Markov chains, compute a pair of new states of the two chains using the same momentum and uniform random variable for acceptance.</p> <p>Parameters:</p> Name Type Description Default <code>Q1</code> <code>Float[Array, ' dim']</code> <p>Position in the first chain</p> required <code>Q2</code> <code>Float[Array, ' dim']</code> <p>Position in the second chain</p> required <code>potential</code> <code>Callable[[Float[Array, ' dim']], Float]</code> <p>Potential function</p> required <code>potential_grad</code> <code>Callable[[Float[Array, ' dim']], Float[Array, ' dim']]</code> <p>Gradient of the potential function</p> required <code>step_size</code> <code>Float</code> <p>Step size for leap-frog integration</p> required <code>num_steps</code> <code>int</code> <p>Number of steps for leap-frog integration</p> required <code>key</code> <code>Key</code> <p>Jax pseudorandom number key</p> required <p>Returns:</p> Type Description <code>tuple[Float[Array, ' dim'], Float[Array, ' dim']]</code> <p>Updated values for <code>Q1</code> and <code>Q2</code>.</p> Notes <p>See Algorithm 1 in https://arxiv.org/abs/1709.00404 for details.</p> Source code in <code>HMC_unbiased/main.py</code> <pre><code>@partial(jax.jit, static_argnums=(2,3))\ndef coupled_HMC_step(Q1: Float[Array, \" dim\"], \n                     Q2: Float[Array, \" dim\"], \n                     potential: Callable[[Float[Array, \" dim\"]], Float],\n                     potential_grad: Callable[[Float[Array, \" dim\"]], \n                                    Float[Array, \" dim\"]], \n                     step_size: Float, \n                     num_steps: int,\n                     key: Key\n            ) -&gt; tuple[Float[Array, \" dim\"], Float[Array, \" dim\"]] :\n    \"\"\" Completes a coupled step of Hamiltonian Monte Carlo.\n\n    Given the current states of two Hamiltonian Monte Carlo Markov chains,\n    compute a pair of new states of the two chains using the same momentum\n    and uniform random variable for acceptance.\n\n    Args:\n        Q1: Position in the first chain\n        Q2: Position in the second chain\n        potential: Potential function\n        potential_grad: Gradient of the potential function\n        step_size: Step size for leap-frog integration\n        num_steps: Number of steps for leap-frog integration\n        key: Jax pseudorandom number key\n\n    Returns:\n        Updated values for `Q1` and `Q2`.\n\n    Notes:\n        See Algorithm 1 in https://arxiv.org/abs/1709.00404 for details.\n    \"\"\"\n    key1, key2 = jax.random.split(key, 2)\n    momentum = jax.random.normal(key1, Q1.shape)\n    U = jax.random.uniform(key2)\n\n    Q1_out = HMC_step(momentum, Q1, U, \n                      potential, potential_grad, \n                      step_size, num_steps)\n\n    Q2_out = HMC_step(momentum, Q2, U, \n                      potential, potential_grad, \n                      step_size, num_steps)\n\n    return Q1_out, Q2_out\n</code></pre>"},{"location":"api/#HMC_unbiased.main.coupled_randomwalk_step","title":"coupled_randomwalk_step","text":"<pre><code>coupled_randomwalk_step(\n    Q1: Float[Array, \" dim\"],\n    Q2: Float[Array, \" dim\"],\n    std: Float,\n    marginal: Callable[[Float[Array, \" dim\"]], Float],\n    key: Key,\n) -&gt; tuple[Float[Array, \" dim\"], Float[Array, \" dim\"]]\n</code></pre> <p>Compute a step of a coupled Metropolis-Hastings random walk.</p> <p>Computes a step of an unbiased random walk Metropolis-Hastings MCMC  algorithm. The proposal distribution comes from the maximal coupling of a pair of Gaussian random variables, guaranteeing a finite meeting time with high probability.</p> <p>Parameters:</p> Name Type Description Default <code>Q1</code> <code>Float[Array, ' dim']</code> <p>State of the first Markov chain</p> required <code>Q2</code> <code>Float[Array, ' dim']</code> <p>State of the second Markov chain</p> required <code>std</code> <code>Float</code> <p>Standard deviation of the proposal Gaussians.</p> required <code>marginal</code> <code>Callable[[Float[Array, ' dim']], Float]</code> <p>Target marginal distribution</p> required <code>key</code> <code>Key</code> <p>Jax pseudorandom number key</p> required <p>Returns:</p> Type Description <code>tuple[Float[Array, ' dim'], Float[Array, ' dim']]</code> <p>The updated pair of states for the Markov chains</p> Source code in <code>HMC_unbiased/main.py</code> <pre><code>@partial(jax.jit, static_argnums=(3,))\ndef coupled_randomwalk_step(Q1: Float[Array, \" dim\"], \n                            Q2: Float[Array, \" dim\"], \n                            std: Float, \n                            marginal: Callable[[Float[Array, \" dim\"]], Float], \n                            key: Key\n            ) -&gt; tuple[Float[Array, \" dim\"], Float[Array, \" dim\"]] :\n    \"\"\"Compute a step of a coupled Metropolis-Hastings random walk.\n\n    Computes a step of an unbiased random walk Metropolis-Hastings MCMC \n    algorithm. The proposal distribution comes from the maximal coupling\n    of a pair of Gaussian random variables, guaranteeing a finite meeting\n    time with high probability.\n\n    Args:\n        Q1: State of the first Markov chain\n        Q2: State of the second Markov chain\n        std: Standard deviation of the proposal Gaussians.\n        marginal: Target marginal distribution\n        key: Jax pseudorandom number key\n\n    Returns:\n        The updated pair of states for the Markov chains \n    \"\"\"\n\n    key1, key2 = jax.random.split(key, 2)\n    Q1_proposed, Q2_proposed = sample_gaussian_max_coupling(Q1, Q2, std, key1)\n\n    # Note, these values may be over 1, but it does not impact the decision\n    Q1_acceptance_probability = marginal(Q1_proposed)/marginal(Q1)\n    Q2_acceptance_probability = marginal(Q2_proposed)/marginal(Q2)\n\n    U = jax.random.uniform(key2)\n\n    Q1_out = jax.lax.select(U &lt; Q1_acceptance_probability,\n                            Q1_proposed, Q1)\n\n    Q2_out = jax.lax.select(U &lt; Q2_acceptance_probability,\n                            Q2_proposed, Q2)\n\n    return (Q1_out, Q2_out)\n</code></pre>"},{"location":"api/#helpers","title":"Helpers","text":""},{"location":"api/#HMC_unbiased.helpers.isotropic_gaussian_pdf","title":"isotropic_gaussian_pdf","text":"<pre><code>isotropic_gaussian_pdf(\n    x: Float[Array, \" dim\"],\n    mu: Float[Array, \" dim\"],\n    std: Float,\n) -&gt; Float\n</code></pre> <p>Compute the pdf of an isotropic Gaussian.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' dim']</code> <p>Evaluation point for the distribution</p> required <code>mu</code> <code>Float[Array, ' dim']</code> <p>Mean of the distribution</p> required <code>std</code> <code>Float</code> <p>Standard deviation for each axis</p> required <p>Returns:</p> Type Description <code>Float</code> <p>The evaluation of the multivariate Gaussian pdf at a point</p> Source code in <code>HMC_unbiased/helpers.py</code> <pre><code>@jax.jit\ndef isotropic_gaussian_pdf(x: Float[Array, \" dim\"], \n                           mu: Float[Array, \" dim\"],\n                           std: Float) -&gt; Float:\n    \"\"\"Compute the pdf of an isotropic Gaussian.\n\n    Args:\n        x: Evaluation point for the distribution\n        mu: Mean of the distribution\n        std: Standard deviation for each axis\n\n    Returns:\n        The evaluation of the multivariate Gaussian pdf at a point\n    \"\"\"\n    squared_distance = -jnp.sum(jnp.square(x - mu))\n    normalized_distance = squared_distance / (2 * std**2)\n    density = jnp.exp(normalized_distance)/ (std**x.shape[0] * (2*jnp.pi)**(x.shape[0]/2))\n    return density\n</code></pre>"},{"location":"api/#HMC_unbiased.helpers.construct_potential","title":"construct_potential","text":"<pre><code>construct_potential(\n    marginal: Callable[[Float[Array, \" dim\"]], Float]\n)\n</code></pre> <p>Convert an unnormalized marginal distribution into a potential function.</p> <p>Given an unnormalized marginal distribution, return a potential function and the gradient of the potential function.</p> <p>Parameters:</p> Name Type Description Default <code>marginal</code> <code>Callable[[Float[Array, ' dim']], Float]</code> <p>Marginal distribution</p> required <p>Returns:</p> Type Description <p>A tuple <code>(potential, potential_grad)</code> of jit compiled functions  representing the potential (log) of the distributiona and the gradient of the potential.</p> Warning <p>Magnitude of the elements of the gradient is clipped to 10 to prevent     issues with numerical instabilities.</p> Source code in <code>HMC_unbiased/helpers.py</code> <pre><code>def construct_potential(marginal: Callable[[Float[Array, \" dim\"]], Float]):\n    \"\"\"Convert an unnormalized marginal distribution into a potential function.\n\n    Given an unnormalized marginal distribution, return a potential function\n    and the gradient of the potential function.\n\n    Args:\n        marginal: Marginal distribution\n\n    Returns:\n        A tuple `(potential, potential_grad)` of jit compiled functions \n            representing the potential (log) of the distributiona and the\n            gradient of the potential.\n\n    Warning:\n        Magnitude of the elements of the gradient is clipped to 10 to prevent\n            issues with numerical instabilities.\n    \"\"\"\n    potential = lambda x: -1 * jnp.log(marginal(x))\n    potential_grad = jax.grad(potential)\n    potential_grad_clipped = lambda x: jnp.clip(potential_grad(x), -10, 10)\n    return jax.jit(potential), jax.jit(potential_grad_clipped)\n</code></pre>"},{"location":"api/#HMC_unbiased.helpers.leapfrog_step","title":"leapfrog_step","text":"<pre><code>leapfrog_step(\n    p: Float[Array, \" dim\"],\n    q: Float[Array, \" dim\"],\n    potential_grad: Callable[\n        [Float[Array, \" dim\"]], Float[Array, \" dim\"]\n    ],\n    step_size: Float,\n) -&gt; tuple[Float[Array, \" dim\"], Float[Array, \" dim\"]]\n</code></pre> <p>Completes a step of leapfrog integation.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Float[Array, ' dim']</code> <p>Momentum of the system</p> required <code>q</code> <code>Float[Array, ' dim']</code> <p>Position of the system</p> required <code>potential_grad</code> <code>Callable[[Float[Array, ' dim']], Float[Array, ' dim']]</code> <p>Gradient of the potential function</p> required <code>step_size</code> <code>Float</code> <p>Integrator Step Size</p> required <p>Returns:</p> Type Description <code>tuple[Float[Array, ' dim'], Float[Array, ' dim']]</code> <p>A tuple <code>(p_out, q_out)</code>, where <code>p_out</code> and <code>q_out</code> are the  updated momentum and position, respectively.</p> Source code in <code>HMC_unbiased/helpers.py</code> <pre><code>@partial(jax.jit, static_argnums=(2,))\ndef leapfrog_step(p: Float[Array, \" dim\"], \n                  q: Float[Array, \" dim\"], \n                  potential_grad: Callable[[Float[Array, \" dim\"]], \n                                            Float[Array, \" dim\"]], \n                  step_size: Float\n                ) -&gt; tuple[Float[Array, \" dim\"], Float[Array, \" dim\"]]:\n    \"\"\"Completes a step of leapfrog integation.\n\n    Args:\n        p: Momentum of the system\n        q: Position of the system\n        potential_grad: Gradient of the potential function\n        step_size: Integrator Step Size\n\n    Returns:\n        A tuple `(p_out, q_out)`, where `p_out` and `q_out` are the \n            updated momentum and position, respectively.\n    \"\"\"\n    p_mid = p - step_size/2 * potential_grad(q)\n    q_out = q + step_size * p_mid\n    p_out = p_mid + step_size/2 * potential_grad(q_out)\n\n    return (p_out, q_out)\n</code></pre>"},{"location":"api/#HMC_unbiased.helpers.hamiltonian","title":"hamiltonian","text":"<pre><code>hamiltonian(\n    p: Float[Array, \" dim\"],\n    q: Float[Array, \" dim\"],\n    potential: Callable[[Float[Array, \" dim\"]], Float],\n) -&gt; Float\n</code></pre> <p>Computes the Hamiltonian given the potential.</p> Source code in <code>HMC_unbiased/helpers.py</code> <pre><code>@partial(jax.jit, static_argnums=(2,))\ndef hamiltonian(p: Float[Array, \" dim\"], \n                q: Float[Array, \" dim\"], \n                potential: Callable[[Float[Array, \" dim\"]], Float]\n            ) -&gt; Float:\n    \"\"\"Computes the Hamiltonian given the potential.\"\"\"\n    return potential(q) + jnp.sum(jnp.square(p))/2\n</code></pre>"},{"location":"api/#HMC_unbiased.helpers.HMC_acceptance","title":"HMC_acceptance","text":"<pre><code>HMC_acceptance(\n    p: Float[Array, \" dim\"],\n    q: Float[Array, \" dim\"],\n    p_proposed: Float[Array, \" dim\"],\n    q_proposed: Float[Array, \" dim\"],\n    potential: Callable[[Float[Array, \" dim\"]], Float],\n) -&gt; Float\n</code></pre> <p>Compute the acceptance probability of the HMC proposal.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Float[Array, ' dim']</code> <p>Original momentum </p> required <code>q</code> <code>Float[Array, ' dim']</code> <p>Original position</p> required <code>p_proposed</code> <code>Float[Array, ' dim']</code> <p>Proposed momentum</p> required <code>q_proposed</code> <code>Float[Array, ' dim']</code> <p>Proposed position</p> required <code>potential</code> <code>Callable[[Float[Array, ' dim']], Float]</code> <p>Potential function</p> required <p>Returns:</p> Type Description <code>Float</code> <p>The probability of acceptance in Hamiltonian Monte Carlo.</p> Source code in <code>HMC_unbiased/helpers.py</code> <pre><code>@partial(jax.jit, static_argnums=(4,))\ndef HMC_acceptance(p: Float[Array, \" dim\"], \n                   q: Float[Array, \" dim\"], \n                   p_proposed: Float[Array, \" dim\"], \n                   q_proposed: Float[Array, \" dim\"], \n                   potential: Callable[[Float[Array, \" dim\"]], Float]\n                ) -&gt; Float:\n    \"\"\" Compute the acceptance probability of the HMC proposal.\n\n    Args:\n        p: Original momentum \n        q: Original position\n        p_proposed: Proposed momentum\n        q_proposed: Proposed position\n        potential: Potential function\n\n    Returns:\n        The probability of acceptance in Hamiltonian Monte Carlo.\n    \"\"\"\n    old_energy = hamiltonian(p, q, potential)\n    new_energy = hamiltonian(p_proposed, q_proposed, potential)\n\n    return jax.numpy.clip(jnp.exp(old_energy - new_energy), a_max=1)\n</code></pre>"},{"location":"api/#HMC_unbiased.helpers.sample_gaussian_max_coupling","title":"sample_gaussian_max_coupling","text":"<pre><code>sample_gaussian_max_coupling(\n    x: Float[Array, \" dim\"],\n    y: Float[Array, \" dim\"],\n    std: Float,\n    key: Key,\n) -&gt; tuple[Float[Array, \" dim\"], Float[Array, \" dim\"]]\n</code></pre> <p>Sample from a max coupling of a pair of Gaussians</p> <p>Given two centers and a shared standard deviation, generate a sample from the maximally coupled distribution with marginal distributions of the two gaussians.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Float[Array, ' dim']</code> <p>Center of the first Gaussian</p> required <code>y</code> <code>Float[Array, ' dim']</code> <p>Center of the second Gaussian</p> required <code>std</code> <code>Float</code> <p>Standard deviation of the distributions</p> required <code>key</code> <code>Key</code> <p>Jax pseudorandom number key</p> required <p>Returns:</p> Type Description <code>tuple[Float[Array, ' dim'], Float[Array, ' dim']]</code> <p>A realization of a pair of Gaussian random variables  with maximal coupling.</p> Source code in <code>HMC_unbiased/helpers.py</code> <pre><code>@jax.jit\ndef sample_gaussian_max_coupling(x: Float[Array, \" dim\"], \n                                 y: Float[Array, \" dim\"], \n                                 std: Float, \n                                 key: Key\n    )-&gt; tuple[Float[Array, \" dim\"], Float[Array, \" dim\"]] :\n    \"\"\"Sample from a max coupling of a pair of Gaussians\n\n    Given two centers and a shared standard deviation, generate\n    a sample from the maximally coupled distribution with marginal\n    distributions of the two gaussians.\n\n    Args:\n        x: Center of the first Gaussian\n        y: Center of the second Gaussian\n        std: Standard deviation of the distributions\n        key: Jax pseudorandom number key\n\n    Returns:\n        A realization of a pair of Gaussian random variables \n            with maximal coupling.\n    \"\"\"\n    subkey, key = jax.random.split(key)\n    x_proposed = x + std*jax.random.normal(subkey, x.shape)\n    x_prob = isotropic_gaussian_pdf(x_proposed, x, std)\n    y_prob = isotropic_gaussian_pdf(x_proposed, y, std)\n\n    subkey, key = jax.random.split(key)\n    W = x_prob * jax.random.uniform(subkey)\n\n    def first_path(args):\n        x_proposed, x_prob, y_prob, key = args\n        return x_proposed, x_proposed\n\n    def second_path(args):\n        x_proposed, x_prob, y_prob, key = args\n        W = -1\n\n        def condition(args):\n            y_proposed, x_prob, y_prob, W, key = args\n            return W &lt;= x_prob\n\n        def inner(args):\n            y_proposed, x_prob, y_prob, W, key = args\n\n            subkey, key = jax.random.split(key)\n            y_proposed = y + std * jax.random.normal(subkey, y.shape)\n            x_prob = isotropic_gaussian_pdf(y_proposed, x, std)\n            y_prob = isotropic_gaussian_pdf(y_proposed, y, std)\n            subkey, key = jax.random.split(key)\n            W = y_prob * jax.random.uniform(subkey)\n            return (y_proposed, x_prob, y_prob, W, key)\n\n        inner_args = (x_proposed, x_prob, y_prob, W, key)\n        y_proposed, x_prob, y_prob, W, key = jax.lax.while_loop(\n            condition, inner, inner_args\n        )\n\n        return x_proposed, y_proposed\n\n    args = (x_proposed, x_prob, y_prob, key)\n    out = jax.lax.cond(W &lt; y_prob, first_path, second_path, args)\n    return out\n</code></pre>"},{"location":"api/#HMC_unbiased.helpers.HMC_step","title":"HMC_step","text":"<pre><code>HMC_step(\n    p: Float[Array, \" dim\"],\n    q: Float[Array, \" dim\"],\n    U: Float,\n    potential: Callable[[Float[Array, \" dim\"]], Float],\n    potential_grad: Callable[\n        [Float[Array, \" dim\"]], Float[Array, \" dim\"]\n    ],\n    step_size: Float,\n    num_steps: int,\n) -&gt; Float[Array, \" dim\"]\n</code></pre> <p>Given the random variables, compute a step of Hamiltonian Monte Carlo.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Float[Array, ' dim']</code> <p>Current momentum</p> required <code>q</code> <code>Float[Array, ' dim']</code> <p>Current position</p> required <code>U</code> <code>Float</code> <p>Uniform random variable to determine acceptance</p> required <code>potential</code> <code>Callable[[Float[Array, ' dim']], Float]</code> <p>Potential function for target distribution</p> required <code>potential_grad</code> <code>Callable[[Float[Array, ' dim']], Float[Array, ' dim']]</code> <p>Gradient of the potential function</p> required <code>step_size</code> <code>Float</code> <p>Step size for the leap-frog integration</p> required <code>num_steps</code> <code>int</code> <p>Number of steps for the leap-frog integration</p> required <p>Returns:</p> Type Description <code>Float[Array, ' dim']</code> <p>A new state variable from a single step of Hamiltonian Monte Carlo.</p> Source code in <code>HMC_unbiased/helpers.py</code> <pre><code>@partial(jax.jit, static_argnums=(3,4))\ndef HMC_step(p: Float[Array, \" dim\"], \n             q: Float[Array, \" dim\"], \n             U: Float, \n             potential: Callable[[Float[Array, \" dim\"]], Float], \n             potential_grad: Callable[[Float[Array, \" dim\"]], \n                                       Float[Array, \" dim\"]],  \n             step_size: Float,\n             num_steps: int\n        ) -&gt; Float[Array, \" dim\"]:\n    \"\"\"Given the random variables, compute a step of Hamiltonian Monte Carlo.\n\n    Args:\n        p: Current momentum\n        q: Current position\n        U: Uniform random variable to determine acceptance\n        potential: Potential function for target distribution\n        potential_grad: Gradient of the potential function\n        step_size: Step size for the leap-frog integration\n        num_steps: Number of steps for the leap-frog integration\n\n    Returns:\n        A new state variable from a single step of Hamiltonian Monte Carlo.\n    \"\"\"\n    def inner(i, args):\n        p, q = args\n        return leapfrog_step(p, \n                             q, \n                             potential_grad, \n                             step_size)\n    p_proposed, q_proposed = jax.lax.fori_loop(0, num_steps, inner, (p, q))\n\n    accept = U &lt; HMC_acceptance(p, q, p_proposed, q_proposed, potential)\n\n    return jax.lax.select(accept, q_proposed, q)\n</code></pre>"}]}